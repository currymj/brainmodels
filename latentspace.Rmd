---
title: "R Notebook"
output: html_notebook
  self_contained: no
---

```{r}
source("R/getdata.R")
library(tidyverse)
library(statnet)
library(abind)
library(pbapply)
```

```{r}
adjmats <- import_data() %>% process_data
networks <- map(adjmats, as.matrix) %>% map(~ as.network(., directed=FALSE))
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
```

Now that we've got all the (undirected) networks in a nice form, let's start fitting them.

```{r}
library(reticulate)
use_condaenv("factorial")
lsm <- import("lsm")
lsm.utils <- import("lsm.utils")
np <- import("numpy")
```

Some training parameters:
```{r}
missing_frac <- 0.25
n_itr <- 500
Ks <- seq(2L,21L,2L)
sigmasq_b <- 1.0
H <- 10L
```

```{r}
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
```

Create random masks:
```{r}
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

Here's a function to close over a model and return a function that returns rows of a data frame.
```{r}
model_prep <- function(model, allAs, masks) {
  for (i in 1:N) {
     model$add_data(allAs[i,,], mask=masks[,,i])
  }
}

make_sampler <- function(model) {
    function(sample_num) {
      model$resample()
      
      if(hasName(model, "hs")) {
        model_list <- list(
          hs=model$hs,
          edgeprobs=map(seq(0L,length(model$As)-1L), model$edge_probabilities)
        )} else {
        model_list <- list(
          edgeprobs=map(seq(0L,length(model$As)-1L), model$edge_probabilities)
        )
      }
      list(
        model=model_list,
      df=data.frame(
        name=model$name,
        iteration=sample_num,
        log_prior=model$log_prior(),
        log_likelihood=model$log_likelihood(),
        hll=model$heldout_log_likelihood()
      ))
  }
}

```

```{r}
bernoulli <- lsm$LatentSpaceModel(V, 0L, name="bernoulli")
model_prep(bernoulli, allAs, masks)
bernoulli$initialize()
```

```{r}
bernoulli_sampler <- make_sampler(bernoulli)
n_iters <- 100
#bernoulli_results <- as_tibble(map_df(1:n_iters, bernoulli_sampler))
bernoulli_results_raw <- pblapply(1:n_iters, bernoulli_sampler)
bernoulli_results <- bernoulli_results_raw %>% map_df(~ .$df) %>% as_tibble
```
```{r}
fit_model <- function(model, n_iters = 50, progress_bar = TRUE) {
  sampler <- make_sampler(model)
  if (progress_bar) {
    result <- pblapply(1:n_iters, sampler)
  } else {
    result <- map(1:n_iters, sampler)
  }
  
  result_models <- map(result, ~ .$model)
  result_df <- map(result, ~ .$df) %>% bind_rows
  
  list(
    model=result_models,
    df=result_df
  )
}
```

Some Bernoulli plots:

```{r}
ggplot(bernoulli_results, aes(iteration, log_likelihood)) + geom_line()
ggplot(bernoulli_results, aes(iteration, hll)) + geom_line()
ggplot(bernoulli_results, aes(iteration, log_prior)) + geom_line()
```

Now to fit the "standard" latent space models:

```{r}
standard_models <- map(Ks[1:2], ~ lsm$LatentSpaceModel(V, ., sigmasq_b=sigmasq_b, name=paste0("standard", .)))
standard_models %>% walk(model_prep) %>% walk(~ .$initialize)
standard_model_results <- pblapply(standard_models, function(x) {fit_model(x, n_iter=10, progress_bar=FALSE)}, cl=4)
```


```{r}
standard_model_df <- bind_rows(standard_model_results)

```

Some plots of the standard model:
```{r}
standard_model_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

Now let's do mixture models (likely slow):

```{r}
mixture_models <- map(Ks[1:3], ~ lsm$MixtureOfLatentSpaceModels(V, .*H, H=H, sigmasq_b=sigmasq_b, name=paste0("mixture", .)))
mixture_models %>% walk(~ model_prep(., allAs, masks)) %>% walk(~ .$initialize)
mixture_model_results <- pblapply(mixture_models, function(x) {fit_model(x, n_iter=30, progress_bar=FALSE)})
```

```{r}
mixture_model_df <- map(mixture_model_results, ~ .$df) %>% bind_rows
```

```{r}
mixture_model_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

We could look at the edge probabilities here but it looks like there's heavy overfitting?

```{r}
mixture_models_sh <- map(Ks[1:3], ~ lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, .*H, H=H, sigmasq_b=sigmasq_b, name=paste0("mixture", .)))
mixture_models_sh %>% walk(model_prep) %>% walk(~ .$initialize)
mixture_model_sh_results <- pblapply(mixture_models_sh, function(x) {fit_model(x, n_iter=30, progress_bar=FALSE)})
```

