---
title: "Modeling Populations of Structural Brain Networks"
author: "Michael Jeremiah Curry"
output: html_document
bibliography: bibliography.bib
---

# Abstract

Diffusion tensor imaging, a means of using MR signal to approximately measure orientation and other properties of nerve fibers in the brain, can be used to map out structural connections between brain regions via white matter pathways. These pathways form networks, and thus ought to be amenable to the mathematical tools of network analysis. However, they have some unusually nice properties that many of these tools do not exploit. Vertices across all individuals can be identified with each other (whereas most network models don't make this assumption), and the presence or absence of edges is reasonably trustworthy (whereas most network models assume this is noisy, or that the observed graph is a subset of the true network). The problem instead becomes dealing with the case where we have many accurate samples from the same network-valued random variable, with a fixed set of vertices. We consider a particular Bayesian nonparametric model for approximating such random variables, presented by Durante, Dunson, and Vogelstein [-@durante_nonparametric_2016] and further analyzed by Linderman and Blei [-@linderman_discussion_2017]. Results on simple simulations are mixed but the model is efficient and shows promise.


# Data and its problems

## Data source

The data come from the Open Connectome Project, specifically the KKI-42 dataset (the same as used in the given papers) [landman cite here]. 21 healthy adult humans were scanned twice each, and the results processed using standard diffusion tensor imaging (DTI) methodology. From the diffusion tensor volumes, tractography was performed (a process where "streamlines" are traced along paths of high diffusion, approximately tracing out the paths of nerve fibers). Each brain volume was registered and segmented according to the Desikan atlas, which divides the human brain into 68 anatomical regions, each of which became a vertex in the resulting networks. As in the existing studies, we deal with unweighted, undirected graphs; in the resulting networks, an edge between any pair of the 68 vertices existed if there was at least one track between the corresponding regions.


## DTI, tractography, and their limitations

Diffusion MRI involves using MR signal to measure the motion of water in tissue in one specific direction. By capturing many so-called diffusion weighted images, it is possible to fit a continuous model of water motion. Typically, the model used is a multivariate normal distribution in 3 dimensions (the same equation governing Brownian motion); the covariance matrix of this normal distribution can be thought of as a tensor, and this technique is known as diffusion tensor imaging, or DTI [basser cite]. The eigenvalues and eigenvectors of this covariance matrix provide useful information about the speed and direction of water movement.

DTI does have limitations, though. In particular, when bundles of nerve fibers cross or touch, the standard DTI model cannot cope, and simply sees water movement restricted in all directions (there are more complex models, not as widely used, for avoiding this limitation). When DTI is used for tractography, this problem is of particular concern. Tractography involves following the direction of the largest eigenvector to trace out "tracks", which in general seem to have something to do with the paths of axons. When bundles of fibers cross or touch, tractography may take the wrong path, or stop entirely. This, along with other problems, have caused the originators of DTI-based tractography to urge researchers to use caution in interpreting their results [thomas cite]. In particular, the very colorful and attractive images produced by tractography are deceptively precise and look too much like real anatomy, and analyses of structural connectivity at a voxel level should probably not be trusted due to the possibility of spurious tracks.

In this case, though, connections are between large brain regions each of which is known to correspond to real anatomy. The parameters for tractography were also fairly conservative. So the problem of spurious tracks is much less severe: we're "averaging out" the noise, in some sense.

## Interesting properties of the data

The properties of the data (identifiability of vertices, and the safe assumption that each individual observation is correct, without spurious edges) are fairly unique, and not that well-studied in network analysis. Rather than trying to fit a graph as the result of some random growth process, or as a model that generated some other observed data, they treat each individual brain network as a single sample from a network-valued random variable, where the identity of vertices matters. This, along with concerns about efficient sampling, motivates the design of their model.

# The model(s)

The model is a mixture of latent space models. Each mixture component represents each vertex as a point in a latent space. Assuming a particular observed network comes from a given mixture component, the probability of an edge between two vertices is proportional to a weighted inner product between these latent points (this inner product constitutes a kernel between vertices), plus some global biases for each edge. Each mixture component is weighted by a probability, and the model is designed so that mixture components that are not needed will tend to have their weights drop to zero.

## Statement of Model Likelihood

\begin{align*}
 &p \left(A_n |
    Z, \{\{x_v^{(h)}\}_{v=1}^V,
  \lambda^{(h)}\}_{h=1}^H, h_n \right) 
  =\\ &\prod_{u=1}^V \prod_{v=1}^{u-1}
  \text{Bern} \left(A_{n,[u,v]} |
    \sigma(z_{u,v} + x_{u}^{(h_n)^\intercal} \Lambda^{(h_n)} x_v^{(h_n)}) \right)
\end{align*}

The variables on the left hand side are defined as follows:

- $A_n$: the $n$th network from the data
- $x_v^{(h)}$: the latent space parameters of vertex $v$ in mixture component $h$.
- $\lambda^{(h)}$: a scaling factor for mixture component $h$
- $Z$: a bias applied for each edge, independent of component
- $h_n$: the mixture component we suppose $A_n$ came from

On the right-hand side:

- $A_n$: the $n$th network
- $h_n$: the mixture component we suppose $A_n$ comes from
- $x_u, x_v$: latent space vectors (per component $h_n$)
- $z_{u,v}$: bias term for connection between vertices $u$ and $v$
- $\Lambda^{h(n)}$: just $\text{diag}(\lambda^{(h_n)})$ (same scaling factors as matrix)
- $\sigma$: just the logistic function (squashes between 0 and 1)

![A plate diagram of the model, taken from Durante et al.](diagram.png)

## Prior distributions

### Latent parameters

The edge biases are Gaussian and the edge features are Gaussian, i.e. we have

$$
Z \thicksim \mathbf{N}_{V(V-1)/2}(\mu, \Sigma)
$$
$$
X_{ij}^{h} \thicksim \mathbf{N}(0, 1)
$$

### Feature weights -- sparsity encouraged

The weights assigned to features have a "multiplicative inverse gamma" prior, as shown below. Essentially, each latent model is encouraged to use as few features as possible; however, full support is still guaranteed, meaning the model remains capable of representing any graph. (Note also that the normal priors on $X$ above are in some sense equivalent to $l_2$ regularization.)

$$
\lambda_r^{(h)} = \prod_{m=1}^r \frac{1}{\nu_m^{(h)}}, \text{ where } \nu_1^{h} \thicksim \Gamma(a_1, 1),\: \nu_{\ge 2}^{(h)} \thicksim \Gamma(a_2, 1)
$$

### Mixture weights

The mixture weights are simply distributed as
$$
(\nu_1, \nu_2, \dots, \nu_h) \sim \text{Dirichlet}\left(\frac{1}{H}, \frac{1}{H}, \dots, \frac{1}{H}\right)
$$

(A Dirichlet distribution essentially returns a normalized vector, whose values function as probabilities for a categorical distribution.)

There are theoretical results for simple mixtures of Gaussians suggesting that the Dirichlet prior is enough to encourage sparsity, reducing the numbers of mixture components. Durante et al only have empirical justifications here, but in practice this does seem to work (see below).

## Sampling -- Gibbs Sampler

The posterior distribution of the parameters given a set of observed networks is intractable to compute, as is usually the case in Bayesian latent variable models. One particular problem is introduced by the use of the sigmoid function to "squash" the inner product into a valid probability parameter for the Bernoulli distribution. They use a technique known as Polya-Gamma data augmentation. This involves parametrizing a Polya-Gamma distribution with the results of the inner product, sampling from this distribution, and then using the results to parametrize a normal distribution which is then sampled from to give the latent feature variables. Iterating these steps is shown to converge on the true posterior, in [cite Polson]. Other than this, the sampler is fairly straightforward.

```{r source,echo=FALSE,message=FALSE,warning=FALSE}
source("R/latentspace_sample.R")
```

# Testing

As mentioned above, there were a few main concerns in testing the model:

- Do the models actually converge in a reasonable amount of time?
- Is sparsity actually encouraged: in other words, can the model adapt to having far too many mixture components ($H$ very large)?
- Can the model distinguish components of simulated and real data, or components of real and modified real data?

## Convergence

The models do seem to converge very quickly, at least in the log-likelihood they assign to the observed data. Crucially, fitting models with many mixture components did not seem to alter the rate of convergence all that much. This is a good sign: over-specifying the number of components doesn't impose a huge computational cost, meaning the model is closer to being truly nonparametric.

```{r import,echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
```

```{r first_constants,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_mixture, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_models_sh <- map(Ks, ~ lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, .*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name=paste0("mixture_shrinkage", .)))

mixture_models_sh %>% walk(~ model_prep(., allAs, masks)) %>% walk(~ .$initialize)
mixture_model_sh_results <- pblapply(mixture_models_sh, function(x) {fit_model(x, n_iter=n_itr, progress_bar=FALSE)})
mixture_model_sh_df <- map(mixture_model_sh_results, ~ .$df) %>% bind_rows
```

```{r,echo=FALSE}
mixture_model_sh_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

## Brain data: assignments

Here are the component assignments sampled at each step for all the observed networks. Note that each column isn't the maximum a posteriori component assignment; instead, it is just a sample from the current posterior distribution. This is why there is some occasional "noise". Were we to compute the actual posterior distribution, it would basically be a histogram of these assignments and so overwhelmingly assign probability to the correct assignment.

```{r, echo=FALSE}
plot_mixture_assignments <- function(mixture_model_results, N, n_itr) {
  assignments_table <- as_tibble(simplify2array(map(mixture_model_results$model, ~ .$hs)))
assignments_table <- assignments_table %>% setNames(map(1:n_itr, paste0))
assignments_table$graph <- 1:N
assignments_df <- gather(assignments_table, iter, cluster, -graph)
assignments_df$cluster <- as.factor(assignments_df$cluster)
assignments_df$iter <- map_int(assignments_df$iter,strtoi)
ggplot(assignments_df, aes(iter, graph, fill=cluster)) + geom_tile()
}
```

```{r plot_mixture_assignments,echo=FALSE}
plot_mixture_assignments(mixture_model_sh_results[[4]], N, n_itr)
```

Next we try some posterior predictive checks, i.e. sampling from the posterior and computing various summary statistics as compared to the observed data. In general, it seems that the posterior is generating graphs that are too dense. Given more computation time, it would be interesting to see whether the number of latent dimensions changes this -- in general, in higher dimensions, everything becomes farther apart.

```{r posterior_predictive, cache=TRUE, warning=FALSE, echo=FALSE}
library(statnet)
library(gridExtra)
library(plyr)
chosen_model <- mixture_models_sh[[4]]
samples <- 1:1000 %>% 
  map(~ chosen_model$generate(keep=FALSE)) %>% 
  map(~ network(., directed=FALSE))

plot_vertex_stats <- function(samples) {
degree_counts <- samples %>% map(~ degree(.)) %>% unlist
between_scores <- samples %>% map(~ betweenness(.)) %>% unlist
close_scores <- samples %>% map(~ closeness(.)) %>% unlist

grid.arrange(histogram(degree_counts), histogram(between_scores), histogram(close_scores))
}

plot_triads <- function(samples) {
triads <- samples %>% 
  map(~ triad.census(., mode="graph")) %>% 
  reduce(~ .x + .y) %>% 
  (function(triad) {triad/sqrt(sum(triad^2))})
barplot(triads)
}

plot_vertex_stats(samples)
plot_triads(samples)

adjmat_samples <-alply(allAs, 1)

plot_vertex_stats(adjmat_samples)
plot_triads(adjmat_samples)
```

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
rand_mats <- array(rbernoulli(graphdims[1]*graphdims[2]*10), dim=c(10,graphdims[1],graphdims[2]))
allAs <- abind(allAs, rand_mats, along=1)
```


```{r random_constants, echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_random_model, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_rand <-  lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_rand4")

model_prep(mixture_model_rand, allAs, masks)
mixture_model_rand$initialize()
mixture_model_rand_results <- fit_model(mixture_model_rand, n_iter=n_itr, progress_bar=FALSE)
mixture_model_rand_df <- mixture_model_rand_results$df
```

## Random graphs

If we add some Bernoulli random graphs into the mix, the random graphs and the real data are correctly split into two clusters (even when fit with 10 latent components.)

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_rand_results, N, n_itr)
```

## Random Changes to Graphs

If we randomly perturb some edges to existing graphs, they are not correctly split into two clusters, although the model does try to cope.

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
rand_mats <- abind(array(rbernoulli(graphdims[1]*graphdims[2]*21,0.75), dim=c(21,graphdims[1],graphdims[2])), array(TRUE,c(21,graphdims[1], graphdims[2])), along=1)
allAs <- allAs & rand_mats
```

```{r,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r perturbed, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_perturbed <- lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_perturbed")
model_prep(mixture_model_perturbed, allAs, masks)
mixture_model_perturbed$initialize()
mixture_model_p_results <- fit_model(mixture_model_perturbed, n_iter=n_itr, progress_bar=FALSE)
mixture_model_p_df <- mixture_model_p_results$df
```

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_p_results, N, n_itr)
```

On the other hand, if we instead perturb a small number of random edges, no difference is detected.

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
maskArray <- array(TRUE, c(21, graphdims[1], graphdims[2]))
maskArray[1:21, 5:10,5:10] <- FALSE
rand_mats <- abind(maskArray | array(rbernoulli(graphdims[1]*graphdims[2]*21,0.75), dim=c(21,graphdims[1],graphdims[2])), array(TRUE,c(21,graphdims[1], graphdims[2])), along=1)
allAs <- allAs & rand_mats
```

```{r,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r p2, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_p2 <- lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_perturbed")
model_prep(mixture_model_p2, allAs, masks)
mixture_model_p2$initialize()
mixture_model_p2_results <- fit_model(mixture_model_p2, n_iter=n_itr, progress_bar=FALSE)
mixture_model_p2_df <- mixture_model_p2_results$df
```

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_p2_results, N, n_itr)
```

# Conclusions

## Is the model viable?

While the results in these simulations could be better, I'm not sure whether or not they reflect on the model. It may be the case that a more exhaustive exploration of the space of hyperparameters is needed. There are strong results in the paper and many interesting theoretical extensions (for example, making modifications to the model to allow for hypothesis testing between two populations, see [cite other dunson paper]).

## Future work

First, it would be interesting to more fully explore the space of hyperparameters. While it seems like the model doesn't depend much on the number of mixture components, it's still not clear how the latent dimension affects things. Furthermore it might be helpful to initialize the model with more informed priors (of the same distributional forms).

Further into the future, it would be very interesting to see if this model could be extended to weighted graphs. In the naive case, replacing the Bernoulli distribution with a Gaussian ought to make things easier, but it seems like that isn't necessarily the right model. Designing a latent variable model would take some thought, and it might be the case that different sampling techniques are needed to efficiently approximate the posterior.

# Bibliography
