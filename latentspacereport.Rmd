---
title: "Modeling Populations of Structural Brain Networks"
author: "Michael Jeremiah Curry"
output: html_document
bibliography: bibliography.bib
---



# Abstract

Diffusion tensor imaging, a means of using MR signal to approximately measure orientation and other properties of nerve fibers in the brain, can be used to map out structural connections between brain regions via white matter pathways. These pathways form networks, and thus ought to be amenable to the mathematical tools of network analysis. However, they have some unusually nice properties that many of these tools do not exploit. Vertices across all individuals can be identified with each other (whereas most network models don't make this assumption), and the presence or absence of edges is reasonably trustworthy (whereas most network models assume this is noisy, or that the observed graph is a subset of the true network). The problem instead becomes dealing with the case where we have many accurate samples from the same network-valued random variable, with a fixed set of vertices. We consider a particular Bayesian nonparametric model for approximating such random variables, presented by Durante, Dunson, and Vogelstein [@durante_nonparametric_2016] and further analyzed by Linderman and Blei [@linderman_discussion_2017]. Results on simple simulations are mixed but the model is efficient and shows promise.


# Data and its problems

## Data source

The data come from the Open Connectome Project, specifically the KKI-42 dataset (the same as used in the given papers) [landman cite here]. 21 healthy adult humans were scanned twice each, and the results processed using standard diffusion tensor imaging (DTI) methodology. From the diffusion tensor volumes, tractography was performed (a process where "streamlines" are traced along paths of high diffusion, approximately tracing out the paths of nerve fibers). Each brain volume was registered and segmented according to the Desikan atlas, which divides the human brain into 68 anatomical regions, each of which became a vertex in the resulting networks. As in the existing studies, we deal with unweighted, undirected graphs; in the resulting networks, an edge between any pair of the 68 vertices existed if there was at least one track between the corresponding regions.


## DTI, tractography, and their limitations

Diffusion MRI involves using MR signal to measure the motion of water in tissue in one specific direction. By capturing many so-called diffusion weighted images, it is possible to fit a continuous model of water motion. Typically, the model used is a multivariate normal distribution in 3 dimensions (the same equation governing Brownian motion); the covariance matrix of this normal distribution can be thought of as a tensor, and this technique is known as diffusion tensor imaging, or DTI [basser cite]. The eigenvalues and eigenvectors of this covariance matrix provide useful information about the speed and direction of water movement.

DTI does have limitations, though. In particular, when bundles of nerve fibers cross or touch, the standard DTI model cannot cope, and simply sees water movement restricted in all directions (there are more complex models, not as widely used, for avoiding this limitation). When DTI is used for tractography, this problem is of particular concern. Tractography involves following the direction of the largest eigenvector to trace out "tracks", which in general seem to have something to do with the paths of axons. When bundles of fibers cross or touch, tractography may take the wrong path, or stop entirely. This, along with other problems, have caused the originators of DTI-based tractography to urge researchers to use caution in interpreting their results [thomas cite]. In particular, the very colorful and attractive images produced by tractography are deceptively precise and look too much like real anatomy, and analyses of structural connectivity at a voxel level should probably not be trusted due to the possibility of spurious tracks.

In this case, though, connections are between large brain regions each of which is known to correspond to real anatomy. The parameters for tractography were also fairly conservative. So the problem of spurious tracks is much less severe: we're "averaging out" the noise, in some sense.

## Interesting properties of the data

The properties of the data (identifiability of vertices, and the safe assumption that each individual observation is correct, without spurious edges) are fairly unique, and not that well-studied in network analysis. Rather than trying to fit a graph as the result of some random growth process, or as a model that generated some other observed data, they treat each individual brain network as a single sample from a network-valued random variable, where the identity of vertices matters. This, along with concerns about efficient sampling, motivates the design of their model.

# The model(s)

The model is a mixture of latent space models. Each mixture component represents each vertex as a point in a latent space. Assuming a particular observed network comes from a given mixture component, the probability of an edge between two vertices is proportional to a weighted inner product between these latent points (this inner product constitutes a kernel between vertices), plus some global biases for each edge. Each mixture component is weighted by a probability, and the model is designed so that mixture components that are not needed will tend to have their weights drop to zero.

## Statement of Model Likelihood

\begin{align*}
 &p \left(A_n |
    Z, \{\{x_v^{(h)}\}_{v=1}^V,
  \lambda^{(h)}\}_{h=1}^H, h_n \right) 
  =\\ &\prod_{u=1}^V \prod_{v=1}^{u-1}
  \text{Bern} \left(A_{n,[u,v]} |
    \sigma(z_{u,v} + x_{u}^{(h_n)^\intercal} \Lambda^{(h_n)} x_v^{(h_n)}) \right)
\end{align*}

The variables on the left hand side are defined as follows:

- $A_n$: the $n$th network from the data
- $x_v^{(h)}$: the latent space parameters of vertex $v$ in mixture component $h$.
- $\lambda^{(h)}$: a scaling factor for mixture component $h$
- $Z$: a bias applied for each edge, independent of component
- $h_n$: the mixture component we suppose $A_n$ came from

On the right-hand side:

- $A_n$: the $n$th network
- $h_n$: the mixture component we suppose $A_n$ comes from
- $x_u, x_v$: latent space vectors (per component $h_n$)
- $z_{u,v}$: bias term for connection between vertices $u$ and $v$
- $\Lambda^{h(n)}$: just $\text{diag}(\lambda^{(h_n)})$ (same scaling factors as matrix)
- $\sigma$: just the logistic function (squashes between 0 and 1)

![A plate diagram of the model, taken from Durante et al.](diagram.png)

## Prior distributions

### Latent parameters

The edge biases are Gaussian and the edge features are Gaussian, i.e. we have

$$
Z \thicksim \mathbf{N}_{V(V-1)/2}(\mu, \Sigma)
$$
$$
X_{ij}^{h} \thicksim \mathbf{N}(0, 1)
$$

### Feature weights -- sparsity encouraged

The weights assigned to features have a "multiplicative inverse gamma" prior, as shown below. Essentially, each latent model is encouraged to use as few features as possible; however, full support is still guaranteed, meaning the model remains capable of representing any graph. (Note also that the normal priors on $X$ above are in some sense equivalent to $l_2$ regularization.)

$$
\lambda_r^{(h)} = \prod_{m=1}^r \frac{1}{\nu_m^{(h)}}, \text{ where } \nu_1^{h} \thicksim \Gamma(a_1, 1),\: \nu_{\ge 2}^{(h)} \thicksim \Gamma(a_2, 1)
$$

### Mixture weights

The mixture weights are simply distributed as
$$
(\nu_1, \nu_2, \dots, \nu_h) \sim \text{Dirichlet}\left(\frac{1}{H}, \frac{1}{H}, \dots, \frac{1}{H}\right)
$$

(A Dirichlet distribution essentially returns a normalized vector, whose values function as probabilities for a categorical distribution.)

There are theoretical results for simple mixtures of Gaussians suggesting that the Dirichlet prior is enough to encourage sparsity, reducing the numbers of mixture components. Durante et al only have empirical justifications here, but in practice this does seem to work (see below).

## Gibbs sampler

- Monte Carlo method
- If you want to see how they derived it, it's in the paper
- Key fact: in each iteration, cluster assignments are sampled first
  - only parameters relevant to the sampled clusters are updated
  - so as sparsity encourages most cluster probabilities to be driven to zero, computation time is not wasted on their parameters
- While the original implementation is not available, Blei and Linderman have theirs on Github.

```{r source,echo=FALSE,message=FALSE,warning=FALSE}
source("R/latentspace_sample.R")
```

# Testing

## Testing methodology

- Fit on DTI data
- Random changes to networks, and/or random graphs from known distributions
- Questions: how fast does convergence occur, and what can model detect?

## Brain data: convergence

```{r import,echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
```

```{r first_constants,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_mixture, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_models_sh <- map(Ks, ~ lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, .*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name=paste0("mixture_shrinkage", .)))

mixture_models_sh %>% walk(~ model_prep(., allAs, masks)) %>% walk(~ .$initialize)
mixture_model_sh_results <- pblapply(mixture_models_sh, function(x) {fit_model(x, n_iter=n_itr, progress_bar=FALSE)})
mixture_model_sh_df <- map(mixture_model_sh_results, ~ .$df) %>% bind_rows
```

```{r,echo=FALSE}
mixture_model_sh_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

## Brain data: assignments

```{r, echo=FALSE}
plot_mixture_assignments <- function(mixture_model_results, N, n_itr) {
  assignments_table <- as_tibble(simplify2array(map(mixture_model_results$model, ~ .$hs)))
assignments_table <- assignments_table %>% setNames(map(1:n_itr, paste0))
assignments_table$graph <- 1:N
assignments_df <- gather(assignments_table, iter, cluster, -graph)
assignments_df$cluster <- as.factor(assignments_df$cluster)
assignments_df$iter <- map_int(assignments_df$iter,strtoi)
ggplot(assignments_df, aes(iter, graph, fill=cluster)) + geom_tile()
}
```

```{r plot_mixture_assignments,echo=FALSE}
plot_mixture_assignments(mixture_model_sh_results[[4]], N, n_itr)
```

Some posterior predictive checks:

```{r posterior_predictive, cache=TRUE, warning=FALSE, echo=FALSE}
library(statnet)
library(gridExtra)
library(plyr)
chosen_model <- mixture_models_sh[[4]]
samples <- 1:1000 %>% 
  map(~ chosen_model$generate(keep=FALSE)) %>% 
  map(~ network(., directed=FALSE))

plot_vertex_stats <- function(samples) {
degree_counts <- samples %>% map(~ degree(.)) %>% unlist
between_scores <- samples %>% map(~ betweenness(.)) %>% unlist
close_scores <- samples %>% map(~ closeness(.)) %>% unlist

grid.arrange(histogram(degree_counts), histogram(between_scores), histogram(close_scores))
}

plot_triads <- function(samples) {
triads <- samples %>% 
  map(~ triad.census(., mode="graph")) %>% 
  reduce(~ .x + .y) %>% 
  (function(triad) {triad/sqrt(sum(triad^2))})
barplot(triads)
}

plot_vertex_stats(samples)
plot_triads(samples)

adjmat_samples <-alply(allAs, 1)

plot_vertex_stats(adjmat_samples)
plot_triads(adjmat_samples)
```

## Brain data: assignments

- Fit with different numbers of components
- All about the same, all ended up converging on assigning all graphs to a single component
- This is good, these graphs are very similar to each other so the model is behaving appropriately.

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
rand_mats <- array(rbernoulli(graphdims[1]*graphdims[2]*10), dim=c(10,graphdims[1],graphdims[2]))
allAs <- abind(allAs, rand_mats, along=1)
```


```{r random_constants, echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_random_model, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_rand <-  lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_rand4")

model_prep(mixture_model_rand, allAs, masks)
mixture_model_rand$initialize()
mixture_model_rand_results <- fit_model(mixture_model_rand, n_iter=n_itr, progress_bar=FALSE)
mixture_model_rand_df <- mixture_model_rand_results$df
```

## Random graphs

- If we add some Bernoulli random graphs into the mix, they are correctly split into two clusters.

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_rand_results, N, n_itr)
```

## Random Changes to Graphs

- If we randomly perturb some edges to existing graphs, they are not correctly split into two clusters

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
rand_mats <- abind(array(rbernoulli(graphdims[1]*graphdims[2]*21,0.75), dim=c(21,graphdims[1],graphdims[2])), array(TRUE,c(21,graphdims[1], graphdims[2])), along=1)
allAs <- allAs & rand_mats
```

```{r,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r perturbed, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_perturbed <- lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_perturbed")
model_prep(mixture_model_perturbed, allAs, masks)
mixture_model_perturbed$initialize()
mixture_model_p_results <- fit_model(mixture_model_perturbed, n_iter=n_itr, progress_bar=FALSE)
mixture_model_p_df <- mixture_model_p_results$df
```

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_p_results, N, n_itr)
```


## Specific Random Changes to Graphs

- If we limit this random perturbation to edges within a small number of vertices, no difference is detected

```{r, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
graphdims <- dim(adjmats[[1]])
maskArray <- array(TRUE, c(21, graphdims[1], graphdims[2]))
maskArray[1:21, 5:10,5:10] <- FALSE
rand_mats <- abind(maskArray | array(rbernoulli(graphdims[1]*graphdims[2]*21,0.75), dim=c(21,graphdims[1],graphdims[2])), array(TRUE,c(21,graphdims[1], graphdims[2])), along=1)
allAs <- allAs & rand_mats
```

```{r,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r p2, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_model_p2 <- lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, 4L*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name="mixture_perturbed")
model_prep(mixture_model_p2, allAs, masks)
mixture_model_p2$initialize()
mixture_model_p2_results <- fit_model(mixture_model_p2, n_iter=n_itr, progress_bar=FALSE)
mixture_model_p2_df <- mixture_model_p2_results$df
```

```{r, echo=FALSE}
plot_mixture_assignments(mixture_model_p2_results, N, n_itr)
```

# Conclusions

## Is the model viable?

- Not very good results on these simple simulations
- It's worth exploring further, and does seem pretty powerful
- Good results in papers
- David Blei approves of it
- I would like to test it on real data with psychological or other covariates, if I had access (replicate result in a paper)

## Future work

- Extension for hypothesis testing -- Dunson and Durante have done this
- What kind of structural changes can it distinguish?
- Posterior predictive checks on network statistics (this one hopefully goes in the final report, it's just computationally intensive)
- Extension to weighted graphs
  - in the naive case, replacing the Bernoulli with a Gaussian should actully make things easier, but that may not be the right model