---
title: "Graph Modeling Presentation"
author: "Michael Jeremiah Curry"
output: beamer_presentation
---

# Introduction

## Network-valued random-variables

- Want a probability distribution over the space of networks
- In practice this means a probability distribution over a space of matrices
- Or for simple undirected graphs, just a space of binary vectors

## Papers

- "Nonparametric Bayes Modeling of Populations of Networks"
  - by Daniele Durante, David B. Dunson, Joshua T. Vogelstein
- A discussion of "Nonparametric Bayes Modeling of Populations of Networks" by Durante, Dunson, and Vogelstein (2016)
  - by Scott W. Linderman and David Blei
  - https://github.com/blei-lab/factorial-network-models

## The model(s)

- Combines two ideas

### Latent space models

- Each vertex is assumed to come from some latent space
- Can think of the latent space as "features"
- Nearness in latent space determines edge probability
- There may be global feature weights
- In this specific case, features are low-rank factorization of the adjacency matrix

### Mixture models

- The whole distribution is a mixture of many such models; first pick a model then draw a graph
- "Which mixture" is a latent variable; we're likely interested in the posterior distribution of this given an observed graph.

# Statement of Model

## Model likelihood

\begin{align*}
 &p \left(A_n |
    Z, \{\{x_v^{(h)}\}_{v=1}^V,
  \lambda^{(h)}\}_{h=1}^H, h_n \right) 
  =\\ &\prod_{u=1}^V \prod_{v=1}^{u-1}
  \text{Bern} \left(A_{n,[u,v]} |
    \sigma(z_{u,v} + x_{u}^{(h_n)^\intercal} \Lambda^{(h_n)} x_v^{(h_n)}) \right)
\end{align*}

## Left-hand side:

$$
p \left(A_n | Z, \{\{x_v^{(h)}\}_{v=1}^V, \lambda^{(h)}\}_{h=1}^H, h_n \right) 
$$

- $A_n$: the $n$th network

- $x_v^{(h)}$: the latent space parameters of vertex $v$ in mixture component $h$.
- $\lambda^{(h)}$: a scaling factor for mixture component $h$
- $Z$: a bias applied for each edge
- $h_n$: the mixture component we suppose $A_n$ came from

## Right-hand side:

$$
\prod_{u=1}^V \prod_{v=1}^{u-1}
  \text{Bern} \left(A_{n,[u,v]} |
    \sigma(z_{u,v} + x_{u}^{(h_n)^\intercal} \Lambda^{(h_n)} x_v^{(h_n)}) \right).
$$
    
- $A_n$: the $n$th network
- $h_n$: the mixture component we suppose $A_n$ comes from
- $x_u, x_v$: latent space vectors (per component $h_n$)
- $z_{u,v}$: bias term for connection between vertices $u$ and $v$
- $\Lambda^{h(n)}$: just $\text{diag}(\lambda^{(h_n)})$ (same scaling factors as matrix)
- $\sigma$: just the logistic function (squashes between 0 and 1)

## Prior distribution -- mixtures

- A "multiplicative inverse gamma", parameters are $a_1$ and $a_2$
- Basically just encourages sparsity (the vector $\lambda^{(h)}$ will have fewer components)
- They do it in this complicated way because it guarantees full support, apparently
$$
\lambda_r^{(h)} = \prod_{m=1}^r \frac{1}{\nu_m^{(h)}}, \text{ where } \nu_1^{h} \thicksim \Gamma(a_1, 1),\: \nu_{\ge 2}^{(h)} \thicksim \Gamma(a_2, 1)
$$

## Prior distribution parameters

- Edge biases are Gaussian: $Z \thicksim \mathbf{N}_{V(V-1)/2}(\mu, \Sigma)$
- So are latent parameters: $X_{vr}^{h} \thicksim \mathbf{N}(0, 1)$

# Fitting

## Gibbs sampler

- Monte Carlo method
- If you want to see how they derived it, it's in the paper
- While the original implementation is not available, Blei and Linderman have theirs on Github.

```{r source,echo=FALSE,message=FALSE,warning=FALSE}
source("R/latentspace_sample.R")
```

## Brain data: convergence

```{r import,echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
```

```{r first_constants,echo=FALSE}
missing_frac <- 0.25
n_itr <- 100
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_mixture, cache=TRUE,warning=FALSE,echo=FALSE}
mixture_models_sh <- map(Ks, ~ lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, .*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name=paste0("mixture_shrinkage", .)))

mixture_models_sh %>% walk(~ model_prep(., allAs, masks)) %>% walk(~ .$initialize)
mixture_model_sh_results <- pblapply(mixture_models_sh, function(x) {fit_model(x, n_iter=n_itr, progress_bar=FALSE)})
mixture_model_sh_df <- map(mixture_model_sh_results, ~ .$df) %>% bind_rows
```

```{r,echo=FALSE}
mixture_model_sh_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

## Brain data: assignments

```{r plot_mixture_assignments,echo=FALSE}
assignments_table <- as_tibble(simplify2array(map(mixture_model_sh_results[[4]]$model, ~ .$hs)))
assignments_table <- assignments_table %>% setNames(map(1:n_itr, paste0))
assignments_table$graph <- 1:N
assignments_df <- gather(assignments_table, iter, cluster, -graph)
assignments_df$cluster <- as.factor(assignments_df$cluster)
assignments_df$iter <- map_int(assignments_df$iter,strtoi)
ggplot(assignments_df, aes(iter, graph, fill=cluster)) + geom_tile()
```

## Brain data: assignments

- Fit with different numbers of components
- All about the same, all ended up converging on assigning all graphs to a single component
- This is good, these graphs are very similar to each other so the model is behaving appropriately.

## Random graphs: convergence
```{r random_constants, echo=FALSE}
adjmats <- import_data() %>% process_data
map(adjmats, as.matrix) %>% abind(along=3) %>% aperm(perm=c(3,1,2)) -> allAs
allAs <- allAs > 0
for (i in 1:10) {
  allAs <- abind(allAs, round(matrix(runif(70*70), 70, 70)), along=1)
}
missing_frac <- 0.25
n_itr <- 300
Ks <- seq(2L,21L,4L)
sigmasq_b <- 1.0
H <- 10L
arrDims <- dim(allAs)
N <- arrDims[1]
Vorig <- arrDims[2]
bad_indices <- c(1,36)
good_indices <- (1:Vorig)[!((1:Vorig) %in% bad_indices)]
allAs <- allAs[, good_indices, good_indices]
V <- Vorig - length(bad_indices)
allAs <- allAs > 0
masks <- replicate(N, lsm.utils$random_mask(V, missing_frac))
```

```{r run_random_model, cache=TRUE,warning=FALSE,echo=FALSE}

mixture_models_rand <- map(Ks, ~ lsm$MixtureOfLatentSpaceModelsWithShrinkage(V, .*H, H=H, sigmasq_b=sigmasq_b, sigmasq_prior_prms=dict(a1=2.5, a2=3.5), name=paste0("mixture_rand", .)))

mixture_models_rand %>% walk(~ model_prep(., allAs, masks)) %>% walk(~ .$initialize)
mixture_model_rand_results <- pblapply(mixture_models_rand, function(x) {fit_model(x, n_iter=n_itr, progress_bar=FALSE)})
mixture_model_rand_df <- map(mixture_model_rand_results, ~ .$df) %>% bind_rows
```


```{r,echo=FALSE}
mixture_model_rand_df %>% ggplot(aes(x=iteration, y=log_likelihood,color=name)) + geom_line()
```

## Random graphs: assignments

- The graphs are correctly split into two clusters.

```{r,echo=FALSE}
assignments_table <- as_tibble(simplify2array(map(mixture_model_rand_results[[4]]$model, ~ .$hs)))
assignments_table <- assignments_table %>% setNames(map(1:n_itr, paste0))
assignments_table$graph <- 1:N
assignments_df <- gather(assignments_table, iter, cluster, -graph)
assignments_df$cluster <- as.factor(assignments_df$cluster)
assignments_df$iter <- map_int(assignments_df$iter,strtoi)
ggplot(assignments_df, aes(iter, graph, fill=cluster)) + geom_tile()
```


## Conclusions

- This model seems to work pretty well, at least in these simple cases
- The goal is ultimately to distinguish populations
  - Given unlabelled data, maybe we can find structure (cluster assignments)
  - Fitting on a population we can compute likelihood of another and hypothesis test
  - Simulate many draws and compare specific derived quantities.
- Questions
  - Can it identify more subtle latent structures?
  - Can we condition on non-network observed variables (i.e. characteristics of individuals)?
